{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01d6351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 周期性序列 ===\n",
      "mode: stay\n",
      "S_upper_bound: 2.321928094887362\n",
      "S_hat(bits/symbol): 0.4703213491478937\n",
      "Pi_max: 0.9361365051008761\n",
      "alphabet_size: 5\n",
      "seq_length: 50\n",
      "\n",
      "=== 按停留点粒度 ===\n",
      "mode: stay\n",
      "S_upper_bound: 3.9068905956085187\n",
      "S_hat(bits/symbol): 2.321928094887362\n",
      "Pi_max: 0.638143818359822\n",
      "alphabet_size: 15\n",
      "seq_length: 5\n",
      "\n",
      "=== 按小时粒度 ===\n",
      "mode: hourly\n",
      "S_upper_bound: 2.0\n",
      "S_hat(bits/symbol): 1.4332889474613348\n",
      "Pi_max: 0.6717894920147955\n",
      "alphabet_size: 4\n",
      "seq_length: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12588\\1755128174.py:112: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  t0 = df[stime_col].min().floor('H')\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12588\\1755128174.py:113: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  t1 = df[etime_col].max().ceil('H')\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12588\\1755128174.py:114: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  hours = pd.date_range(t0, t1, freq='H')\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12588\\1755128174.py:118: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  h = s.floor('H')\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===============================================================\n",
    "# 1. Lempel–Ziv 熵率计算\n",
    "# ===============================================================\n",
    "def lz_lambdas(seq):\n",
    "    \"\"\"计算每个位置的 Lambda_i（最短未在前缀出现的子串长度）\"\"\"\n",
    "    N = len(seq)\n",
    "    seq_t = tuple(seq)\n",
    "    lambdas = []\n",
    "    for i in range(N):\n",
    "        max_L = N - i\n",
    "        L = 1\n",
    "        while L <= max_L:\n",
    "            subseq = seq_t[i:i+L]\n",
    "            if L > i:\n",
    "                lambdas.append(L)\n",
    "                break\n",
    "            occurred = False\n",
    "            for j in range(0, i - L + 1):\n",
    "                if seq_t[j:j+L] == subseq:\n",
    "                    occurred = True\n",
    "                    break\n",
    "            if not occurred:\n",
    "                lambdas.append(L)\n",
    "                break\n",
    "            L += 1\n",
    "        else:\n",
    "            lambdas.append(max_L)\n",
    "    return lambdas\n",
    "\n",
    "\n",
    "def lz_entropy_rate(seq):\n",
    "    \"\"\"计算 Lempel–Ziv 熵率（bits/符号）\"\"\"\n",
    "    N = len(seq)\n",
    "    if N == 0:\n",
    "        return 0.0, []\n",
    "    lambdas = lz_lambdas(seq)\n",
    "    sum_lambda = sum(lambdas)\n",
    "    if sum_lambda == 0:\n",
    "        return 0.0, lambdas\n",
    "    H_hat = (N * math.log2(N)) / sum_lambda\n",
    "    return H_hat, lambdas\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 2. 修正版 Fano 方程 — 理论最大可预测性 Πmax\n",
    "# ===============================================================\n",
    "def predictability_upper_bound(S_bits_per_symbol, alphabet_size, tol=1e-9, max_iter=200):\n",
    "    \"\"\"根据 Song et al. (2010) 公式计算理论最大可预测性 Πmax\"\"\"\n",
    "    if S_bits_per_symbol < 1e-12:\n",
    "        return 1.0\n",
    "    H_max = math.log2(alphabet_size)\n",
    "    if abs(S_bits_per_symbol - H_max) < 1e-12:\n",
    "        return 1.0 / alphabet_size\n",
    "\n",
    "    def f(Pi):\n",
    "        Pi = np.clip(Pi, 1e-12, 1 - 1e-12)\n",
    "        return -Pi * math.log2(Pi) - (1 - Pi) * math.log2((1 - Pi) / (alphabet_size - 1)) - S_bits_per_symbol\n",
    "\n",
    "    low, high = 0.0, 1.0\n",
    "    for _ in range(max_iter):\n",
    "        mid = 0.5 * (low + high)\n",
    "        if f(mid) < 0:  # 修正方向：f(Pi) 单调递减\n",
    "            high = mid\n",
    "        else:\n",
    "            low = mid\n",
    "        if abs(high - low) < tol:\n",
    "            break\n",
    "    return (low + high) / 2\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 3. 停留时长分箱 + 联合符号\n",
    "# ===============================================================\n",
    "def duration_to_bin(duration_sec, bins=None):\n",
    "    if bins is None:\n",
    "        # 默认分箱：0–10min,10–30min,30–60min,1–3h,>3h\n",
    "        bins = [0, 600, 1800, 3600, 10800, float('inf')]\n",
    "    for i in range(len(bins)-1):\n",
    "        if bins[i] <= duration_sec < bins[i+1]:\n",
    "            return i\n",
    "    return len(bins)-2\n",
    "\n",
    "\n",
    "def build_stay_sequence(df, grid_col='grid', duration_col='duration', bins=None):\n",
    "    \"\"\"构建按停留点粒度的联合符号序列\"\"\"\n",
    "    if bins is None:\n",
    "        bins = [0, 600, 1800, 3600, 10800, float('inf')]\n",
    "    grids = sorted(df[grid_col].unique())\n",
    "    grid_to_idx = {g: i for i, g in enumerate(grids)}\n",
    "    B = len(bins) - 1\n",
    "    seq = []\n",
    "    for _, row in df.iterrows():\n",
    "        g = grid_to_idx[int(row[grid_col])]\n",
    "        d = duration_to_bin(float(row[duration_col]), bins)\n",
    "        symbol = g * B + d\n",
    "        seq.append(symbol)\n",
    "    return seq, len(grids) * B\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 4. 按小时展开轨迹序列\n",
    "# ===============================================================\n",
    "def build_hourly_sequence(df, stime_col='stime', etime_col='etime', grid_col='grid', fill_missing=-1):\n",
    "    \"\"\"将轨迹展开为每小时一个 grid\"\"\"\n",
    "    df = df.copy()\n",
    "    df[stime_col] = pd.to_datetime(df[stime_col])\n",
    "    df[etime_col] = pd.to_datetime(df[etime_col])\n",
    "    t0 = df[stime_col].min().floor('H')\n",
    "    t1 = df[etime_col].max().ceil('H')\n",
    "    hours = pd.date_range(t0, t1, freq='H')\n",
    "    hour_grid = {h: fill_missing for h in hours}\n",
    "    for _, row in df.iterrows():\n",
    "        s, e, g = row[stime_col], row[etime_col], int(row[grid_col])\n",
    "        h = s.floor('H')\n",
    "        while h < e:\n",
    "            if h in hour_grid:\n",
    "                hour_grid[h] = g\n",
    "            h += pd.Timedelta(hours=1)\n",
    "    seq = [hour_grid[h] for h in hours]\n",
    "    return seq, len(set(seq))\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 5. 主函数：计算两种粒度的可预测性\n",
    "# ===============================================================\n",
    "def compute_predictability(data, mode='stay'):\n",
    "    \"\"\"\n",
    "    mode: 'stay' (按停留点计算) 或 'hourly' (按小时展开)\n",
    "    data:\n",
    "      - 若为 list，则直接作为序列（如 grid 序列）\n",
    "      - 若为 DataFrame，则从中构建序列\n",
    "    返回: dict 包含 S_hat, Pi_max, alphabet_size, seq_length\n",
    "    \"\"\"\n",
    "    # 直接传入序列\n",
    "    if isinstance(data, (list, np.ndarray)):\n",
    "        seq = [int(x) for x in data]\n",
    "        alphabet_size = len(set(seq))\n",
    "    # 传入 DataFrame\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        if mode == 'stay':\n",
    "            seq, alphabet_size = build_stay_sequence(data)\n",
    "        elif mode == 'hourly':\n",
    "            seq, alphabet_size = build_hourly_sequence(data)\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'stay' or 'hourly'\")\n",
    "        seq = [int(s) for s in seq if s != -1]\n",
    "    else:\n",
    "        raise TypeError(\"data must be a list or pandas.DataFrame\")\n",
    "    \n",
    "    # alphabet_size = 21000\n",
    "    S_hat, _ = lz_entropy_rate(seq)\n",
    "    S_upper_bound = math.log2(alphabet_size)\n",
    "    Pi_max = predictability_upper_bound(S_hat, alphabet_size)\n",
    "\n",
    "    return {\n",
    "        \"mode\": mode,\n",
    "        \"S_upper_bound\":S_upper_bound,\n",
    "        \"S_hat(bits/symbol)\": S_hat,\n",
    "        \"Pi_max\": Pi_max,\n",
    "        \"alphabet_size\": alphabet_size,\n",
    "        \"seq_length\": len(seq)\n",
    "    }\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 示例\n",
    "# ===============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # ==== 示例1：规律性很高的序列 ====\n",
    "    seq = [123, 4567, 4432, 2124, 4521] * 10\n",
    "    res_seq = compute_predictability(seq)\n",
    "    print(\"=== 周期性序列 ===\")\n",
    "    for k, v in res_seq.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # ==== 示例2：停留点轨迹数据 ====\n",
    "    data = [\n",
    "        [1,'001','2008-10-23 05:59:59','2008-10-23 10:32:53',116.325033,39.979672,16374.0,14806],\n",
    "        [1,'001','2008-10-23 11:03:36','2008-10-23 23:49:59',116.306654,40.016016,45983.0,14621],\n",
    "        [1,'001','2008-10-24 00:14:57','2008-10-24 01:48:27',116.325005,39.978826,5610.0,14806],\n",
    "        [1,'001','2008-10-24 01:54:17','2008-10-24 03:21:04',116.310839,39.980082,5207.0,14617],\n",
    "        [1,'001','2008-10-24 03:54:19','2008-10-24 05:30:43',116.325061,39.979697,5784.0,14806],\n",
    "    ]\n",
    "    df = pd.DataFrame(data, columns=['userID','col2','stime','etime','lon','lat','duration','grid'])\n",
    "\n",
    "    print(\"\\n=== 按停留点粒度 ===\")\n",
    "    res_stay = compute_predictability(df, mode='stay')\n",
    "    for k, v in res_stay.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    print(\"\\n=== 按小时粒度 ===\")\n",
    "    res_hourly = compute_predictability(df, mode='hourly')\n",
    "    for k, v in res_hourly.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b37cede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: stay\n",
      "S_upper_bound: 8.022367813028454\n",
      "S_hat(bits/symbol): 3.046651980015867\n",
      "Pi_max: 0.7256921394728124\n",
      "alphabet_size: 260\n",
      "seq_length: 499\n"
     ]
    }
   ],
   "source": [
    "# 单个用户实际停留点数据。\n",
    "traj = pd.read_csv('./Data/Output/Stays/000.csv', index_col=0)\n",
    "traj.head(3)\n",
    "\n",
    "res_stay = compute_predictability(traj, mode='stay')\n",
    "for k, v in res_stay.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1701cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: stay\n",
      "S_upper_bound: 14.358101707440847\n",
      "S_hat(bits/symbol): 2.1291235562856454\n",
      "Pi_max: 0.8871288537047803\n",
      "alphabet_size: 21000\n",
      "seq_length: 27919\n"
     ]
    }
   ],
   "source": [
    "# 实际停留点数据。\n",
    "traj = pd.read_csv('./Data/Output/AllUserTimeSeries.csv', index_col=0)\n",
    "traj.head(3)\n",
    "\n",
    "res_stay = compute_predictability(traj, mode='stay')\n",
    "for k, v in res_stay.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# consume time 4m 30.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e41fc264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1321"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = pd.read_csv('./Data/Output/AllUserTimeSeries.csv', index_col=0)\n",
    "vocabulary_grids_count = len(set(vocabulary['grid'].to_list()))\n",
    "vocabulary_grids_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
